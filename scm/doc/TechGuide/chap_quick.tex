\chapter{Quick Start Guide}
\label{chapter: quick}

This chapter provides instructions for obtaining and compiling the CCPP SCM. The SCM code calls CCPP-compliant physics schemes through the CCPP framework code. As such, it requires the CCPP framework code and physics code, both of which are included as submodules within the SCM code. This package can be considered a simple example for an atmospheric model to interact with physics through the CCPP. 

Alternatively, if one doesn't have access or care to set up a machine with the appropriate system requirements but has a working Docker installation, it is possible to create and use a Docker container with a pre-configured computing environment with a pre-compiled model. This is also an avenue for running this software with a Windows PC. See section \ref{docker} for more information.

\section{Obtaining Code}
\label{obtaining_code}

The source code for the CCPP and SCM is provided through GitHub.com.  This release branch contains the tested and supported version for general use, while a development branch is less stable, yet contains the latest developer code. Instructions for using either option are discussed here.

\subsection{Release Code}

Clone the source using
\begin{lstlisting}[language=bash]
git clone --recursive -b release/public-v4 https://github.com/NCAR/gmtb-scm
\end{lstlisting}
             Recall that the \execout{recursive} option in this command clones the main gmtb-scm repository and all subrepositories (ccpp-physics and ccpp-framework). Using this option, there is no need to execute \exec{git submodule init} and \exec{git submodule update}.

The CCPP framework can be found in the ccpp/framework subdirectory at this level.  The CCPP physics parameterizations can be found in the ccpp/physics subdirectory.

\subsection{Development Code}

If you would like to contribute as a developer to this project, please see (in addition to the rest of this guide) the scientific and technical documentation included with this release:

\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/documentation}

There you will find links to all of the documentation pertinent to developers.

For working with the development branches (stability not guaranteed), check out the \exec{dtc/develop} branches of the repository (and submodules):
\begin{lstlisting}[language=bash]
git clone --recursive -b dtc/develop https://github.com/NCAR/gmtb-scm
\end{lstlisting}
You may want to double-check that the dtc/develop branch of the SCM is pointing to the latest commits of the dtc/develop branches of ccpp-physics and ccpp-framework. While we update the submodule pointers often, it is occasionally forgotten. To ensure that you have the latest development code for the submodules, execute the following:
\begin{enumerate}
\item Navigate to the ccpp-physics directory.
\begin{lstlisting}[language=bash]
cd gmtb-scm/ccpp/physics
\end{lstlisting}
\item Check out the right branch (cloning recursively as instructed above creates a ``detached head'' state for the submodules by default).
\begin{lstlisting}[language=bash]
git checkout dtc/develop
\end{lstlisting}
\item Pull down the latest changes just to be sure.
\begin{lstlisting}[language=bash]
git pull
\end{lstlisting}
\item Do the same for ccpp-framework
\begin{lstlisting}[language=bash]
cd ../framework
git checkout dtc/develop
git pull
\end{lstlisting}
\item Change back to the main directory for following the instructions in section \ref{section: compiling} assuming system requirements in section \ref{section: systemrequirements} are met.
\begin{lstlisting}[language=bash]
cd ../..
\end{lstlisting}
\end{enumerate}


\section{System Requirements, Libraries, and Tools}
\label{section: systemrequirements}

The source code for the SCM and CCPP component is in the form of programs written in FORTRAN, FORTRAN 90, and C. In addition, the I/O relies on the netCDF libraries. Beyond the standard scripts, the build system relies on use of the Python scripting language, along with cmake, GNU make and date.

The basic requirements for building and running the CCPP and SCM bundle are listed below. The versions listed reflect successful tests and there is no guarantee that the code will work with different versions.
\begin{itemize}
    \item FORTRAN 90+ compiler
    	\begin{itemize}
   	 \item ifort 18.0.5.274 and 19.0.2
	 \item gfortran 6.2, 8.3, and 9.1
	 \end{itemize}
    \item C compiler
    	\begin{itemize}
	\item icc 18.0.5.274 and 19.0.2
	\item gcc 6.2 and 8.3
	\item Apple Clang 11.0.0.11000033
	\end{itemize}
    \item cmake 2.8.12.1, 2.8.12.2, 3.6.2, 3.16.3, 3.16.4
    	\begin{itemize}
	\item NOTE: Version 3.15+ is required if installing NCEPLIBS
	\end{itemize}
   	 \item netCDF 4.3.0, 4.4.0, 4.4.1.1, 4.5.0, 4.6.1, 4.6.3, 4.7.0, 4.7.3 (not 3.x) with HDF5, ZLIB and SZIP
    \item Python 2.7.5, 2.7.9, 2.7.13, and 2.7.16 (not 3.x)
    \item Libxml2 2.2, 2.9.7, 2.9.9
\end{itemize}

Because these tools and libraries are typically the purview of system administrators to install and maintain, they are considered  part of the basic system requirements.

Further, there are several utility libraries as part of the NCEPlibs package that must be installed with environment variables pointing to their locations prior to building the SCM.
\begin{itemize}
    \item bacio - Binary I/O Library
    \item sp - Spectral Transformation Library
    \item w3nco - GRIB decoder and encoder library
\end{itemize}
The following environment variables are used by the build system to properly link these libraries: \execout{BACIO\_LIB4}, \execout{SP\_LIBd}, and \execout{W3NCO\_LIBd}.  These libraries are prebuilt on most NOAA machines using the Intel compiler and on Cheyenne for the Intel and GNU compilers. The machine setup scripts mentioned in section \ref{section: compiling} load these libraries (which are identical to those used by the UFS Medium Range Weather Application on those machines) and set these environment variables for the user automatically. If one has previously installed this prerequisite software as part of the UFS on the same machine where the SCM will be installed and used, just make sure that the environment variables listed above point to the installations used for the UFS.

For those needing to build the libraries themselves, the repository contains a bash installation script in \execout{gmtb-scm/contrib/build\_nceplibs.sh} that will fetch the source code from their authoritative repositories on GitHub and install them locally for the SCM to use. To execute this script, perform the following step from the top level directory (\execout{gmtb-scm}). 

\begin{lstlisting}
./contrib/build_nceplibs.sh $PWD/nceplibs
\end{lstlisting}

Following successful execution of this script, the commands to set the proper environment variables mentioned above will be written to the terminal as output. One must execute the correct set for the active shell to finish the installation, e.g., for bash
\begin{lstlisting}
export BACIO_LIB4=/path/to/gmtb-scm/nceplibs/lib/libbacio_v2.2.0_4.a
export SP_LIBd=/path/to/gmtb-scm/nceplibs/lib/libsp_v2.1.0_d.a
export W3NCO_LIBd=/path/to/gmtb-scm/nceplibs/lib/libw3nco_v2.1.0_d.a
\end{lstlisting}
and for t/csh
\begin{lstlisting}
setenv BACIO_LIB4 /path/to/gmtb-scm/nceplibs/lib/libbacio_v2.2.0_4.a
setenv SP_LIBd /path/to/gmtb-scm/nceplibs/lib/libsp_v2.1.0_d.a
setenv W3NCO_LIBd /path/to/gmtb-scm/nceplibs/lib/libw3nco_v2.1.0_d.a
\end{lstlisting}

The installation of NCEPLIBS requires \execout{cmake} v3.15+. There are many ways to obtain the required version, either by following instructions provided by \execout{cmake} (\url{https://cmake.org/install/}), or by following the instructions provided for the UFS release pertaining to the NCEPLIBS-external repository (\url{https://ufs-weather-model.readthedocs.io/en/ufs-v1.0.0/BuildingAndRunning.html#prerequisite-libraries}). If you already have a working version of \execout{cmake}, you can leverage it to install a newer version (local to the SCM) by following these instructions (starting from the top level \execout{gmtb-scm} directory).

\begin{lstlisting}
mkdir -p cmake
(curl -L -O https://github.com/Kitware/CMake/releases/download/v3.16.5/cmake-3.16.5.tar.gz) the text in parentheses is one command
tar -xf cmake-3.16.5.tar.gz
cd cmake-3.16.5
(cmake -DCMAKE_USE_OPENSSL=OFF -DCMAKE_INSTALL_PREFIX=/path/to/gmtb-scm/cmake  . ) the text in parentheses is one command
make install
cd .. 
rm -rfd cmake-3.16.5 
rm -rfd cmake-3.16.5-tar-gz
\end{lstlisting}

Prepend this installation of \execout{cmake} to your path environment variable to use it for building the NCEPLIBS.

For bash
\begin{lstlisting}
export PATH=/path/to/gmtb-scm/cmake/bin:$PATH
\end{lstlisting}
and for t/csh
\begin{lstlisting}
setenv PATH /path/to/gmtb-scm/cmake/bin:$PATH
\end{lstlisting}

\subsection{Compilers}
The CCPP and SCM have been tested on a variety of
computing platforms. Currently the CCPP system is actively supported
on Linux and MacOS computing platforms using the Intel or GNU Fortran
compilers. Please use versions listed in the previous section as unforeseen
build issues may occur when using older compiler versions. Typically the best results come from using the
most recent version of a compiler. If you have problems with compilers, please check the ``Known Issues'' section of the
community website (\url{https://dtcenter.org/gmtb/users/ccpp/support/CCPP_KnownIssues.php}).

\section{Compiling SCM with CCPP}
\label{section: compiling}
The first step in compiling the CCPP and SCM is to properly setup your user environment. Platform-specific scripts are provided to load modules and set the user environment for common platforms.  If you are not using one of these platforms, you will need to set up the same environment on your platform. Following this step, the top level build system will use \execout{cmake} to query system parameters, execute the CCPP prebuild script to match the physics variables (between what the host model -- SCM -- can provide and what is needed by physics schemes in the CCPP), and build the physics caps needed to use them. Finally, \execout{make} is used to compile the components. 
\begin{enumerate}
 \item From the top-level code directory (\execout{gmtb-scm} by default), change directory to the top-level SCM directory.
\begin{lstlisting}[language=bash]
cd scm
\end{lstlisting}
\item If using one of the supported systems, run the appropriate machine setup script. This script loads compiler modules (Fortran 2003-compliant), netCDF module, python environment, etc. and sets compiler environment variables. For \textit{t/csh} shells,
\begin{lstlisting}[language=csh]
source etc/Hera_setup_intel.csh
source etc/Cheyenne_setup_gnu.csh
source etc/Cheyenne_setup_intel.csh
source etc/UBUNTU_setup.csh
source etc/CENTOS_setup.csh
source etc/MACOSX_setup.csh
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
. etc/Hera_setup_intel.sh
. etc/Cheyenne_setup_gnu.sh
. etc/Cheyenne_setup_intel.sh
. etc/UBUNTU_setup.sh
. etc/CENTOS_setup.sh
. etc/MACOSX_setup.sh
\end{lstlisting}
\emph{Note:} If using a local Linux or Mac system, we provide instructions for how to set up your development system (compilers and libraries) in \execout{doc/README\_\{MACOSX,UBUNTU,CENTOS\}.txt}. If following these, you will need to run the respective setup script listed above. If your computing environment was previously set up to use modern compilers with an associated netCDF installation, it may not be necessary, although we recommend setting environment variables such as \execout{CC} and \execout{FC}. \textbf{For version 3.0 and above, it is required to have the \execout{NETCDF} environment variable set to the path of the netCDF installation that was compiled with the same compiler used in the following steps}. Otherwise, the \execout{cmake} step will not complete successfully.

    \item Make a build directory and change into it.
\begin{lstlisting}[language=bash]
mkdir bin && cd bin
\end{lstlisting}

 \item Invoke \exec{cmake} on the source code to build using one of the options below.
\begin{itemize}
\item Default mode
\begin{lstlisting}[language=bash]
cmake ../src
\end{lstlisting}
\item The statements above can be modified with the following options (put before \execout{../src}):
\begin{itemize}
\item Use threading with openmp
\begin{lstlisting}[language=bash]
-DOPENMP=ON
\end{lstlisting}
\item Debug mode
\begin{lstlisting}[language=bash]
-DCMAKE_BUILD_TYPE=Debug
\end{lstlisting}
\end{itemize}
\end{itemize}

Cmake automatically runs the CCPP prebuild script to match required physics variables with those available from the dycore (SCM) and to generate physics caps and makefile segments. It generates software caps for each physics group defined in the supplied SDFs and generates a static library that becomes part of the SCM executable. Appropriate software caps \textbf{will be generated for all suites defined in the \execout{gmtb-scm/ccpp/suites} directory automatically.}

If necessary, the CCPP prebuild script can be executed manually from the top level directory (\execout{gmtb-scm}). The basic syntax is
\begin{lstlisting}[language=bash]
./ccpp/framework/scripts/ccpp_prebuild.py --config=./ccpp/config/ccpp_prebuild_config.py --static --suites=SCM_GFS_v15p2,SCM_GFS_v16beta,SCM_GSD_v1[...] --builddir=./scm/bin [--debug]
\end{lstlisting}
where the argument supplied via the \execout{-{}-suites} variable is a comma-separated list of suite names that exist in the \execout{./ccpp/suites} directory. Note that suite names are the suite definition filenames minus the \exec{suite\_} prefix and \exec{.xml} suffix.
   
       \item If cmake cannot find \execout{libxml2} because it is installed in a non-standard location, add
\begin{lstlisting}[language=bash]
-DPC_LIBXML_INCLUDEDIR=... -DPC_LIBXML_LIBDIR=...
\end{lstlisting}
    to the cmake command. If a compilation error appears related to \execout{libxml2}, namely that a unicode file is not found in the include path, add the appropriate include path to the \execout{CFLAGS} environment variable
\begin{lstlisting}[language=bash]
export CFLAGS="-I/include_path"
\end{lstlisting} before cleaning out the \execout{bin} directory and rerunning \execout{cmake}.
    \item Compile. Add \execout{VERBOSE=1} to obtain more information on the build process.
\begin{lstlisting}[language=bash]
make
\end{lstlisting}
\end{enumerate}

The resulting executable may be found at \execout{./gmtb-scm} (Full path of \execout{gmtb-scm/scm/bin/gmtb-scm}). Depending on the system, it may be necessary to add the location of the CCPP framework and physics libraries to \execout{LD\_LIBRARY\_PATH} to run \execout{./gmtb-scm} (see next section).

Although \execout{make clean} is not currently implemented, an out-of-source build is used, so all that is required to clean the build/run directory is (from the \execout{bin} directory)
\begin{lstlisting}[language=bash]
pwd #confirm that you are in the gmtb-scm/scm/bin directory before deleting files
rm -rfd *
\end{lstlisting}
Note: This command can be dangerous (deletes files without confirming), so make sure that you're in the right directory before executing!

If you encounter errors, please capture a log file from all of the steps, and contact the helpdesk at: \url{gmtb-help@ucar.edu}

\section{Run the SCM with a supplied case}
There are several test cases provided with this version of the SCM. For all cases, the SCM will go through the time steps, applying forcing and calling the physics defined in the chosen suite definition file using physics configuration options from an associated namelist. The model is executed through one of two Python run scripts that are pre-staged into the \execout{bin} directory: \execout{run\_gmtb\_scm.py} or \execout{multi\_run\_gmtb\_scm.py}. The first sets up and runs one integration while the latter will set up and run several integrations serially. 

\subsection{Single Run Script Usage} \label{subsection: singlerunscript}
Running a case requires three pieces of information: the case to run (consisting of initial conditions, geolocation, forcing data, etc.), the physics suite to use (through a CCPP suite definition file), and a physics namelist (that specifies configurable physics options to use). As discussed in chapter \ref{chapter: cases}, cases are set up via their own namelists in \execout{../etc/case\_config}. A default physics suite is provided as a user-editable variable in the script and default namelists are associated with each physics suite (through \execout{../src/default\_namelists.py}), so, technically, one must only specify a case to run with the SCM. The single run script's interface is described below.

\begin{lstlisting}[language=bash]
./run_gmtb_scm.py -c CASE_NAME [-s SUITE_NAME] [-n PHYSICS_NAMELIST_WITH_PATH] [-g] [-d]
\end{lstlisting}

When invoking the run script, the only required argument is the name of the case to run. The case name used must match one of the case configuration files located in \execout{../etc/case\_config} (\emph{without the .nml extension!}). If specifying a suite other than the default, the suite name used must match the value of the suite name in one of the suite definition files located in \execout{../../ccpp/suites} (Note: not the filename of the suite definition file). As part of the fourth CCPP release, the following suite names are valid:
\begin{enumerate}
\item SCM\_GFS\_v15p2
\item SCM\_GFS\_v16beta
\item SCM\_GFS\_v15p2\_no\_nsst
\item SCM\_GFS\_v16beta\_no\_nsst
\item SCM\_csawmg
\item SCM\_GSD\_v1
\end{enumerate}

Note that using the Thompson microphysics scheme (as in SCM\_GSD\_v1) requires the computation of look-up tables during its initialization phase. As of the release, this process has been prohibitively slow with this model, so it is HIGHLY suggested that these look-up tables are downloaded and staged to use this scheme (and the SCM\_GSD\_v1 suite). Pre-computed tables have been created and are available for download at the following URLs:
\begin{itemize}
\item \url{https://dtcenter.org/GMTB/freezeH2O.dat} (243 M)
\item \url{https://dtcenter.org/GMTB/qr_acr_qg.dat} (49 M)
\item \url{https://dtcenter.org/GMTB/qr_acr_qs.dat} (32 M)
\end{itemize}
These files should be staged in \execout{gmtb-scm/scm/data/physics\_input\_data} prior to executing the run script. Since binary files can be system-dependent (due to endianness), it is possible that these files will not be read correctly on your system. For reference, the linked files were generated on Theia using the Intel v18 compiler.

Also note that some cases require specified surface fluxes. Special suite definition files that correspond to the suites listed above have been created and use the \execout{*\_prescribed\_surface} decoration. It is not necessary to specify this filename decoration when specifying the suite name. If the \execout{spec\_sfc\_flux} variable in the configuration file of the case being run is set to \execout{.true.}, the run script will automatically use the special suite definition file that corresponds to the chosen suite from the list above.

If specifying a namelist other than the default, the value must be an entire filename that exists in \execout{../../ccpp/physics\_namelists}. Caution should be exercised when modifying physics namelists since some redundancy between flags to control some physics parameterizations and scheme entries in the CCPP suite definition files currently exists. Values of numerical parameters are typically OK to change without fear of inconsistencies. Lastly, the \execout{-g} flag can be used to run the executable through the \exec{gdb} debugger (assuming it is installed on the system), and the \execout{-d} flag is required when running this command in a Docker container in order to successfully mount a volume between the host machine and the Docker container instance and to share the output and plots with the host machine.

If the run aborts with the error message
\begin{lstlisting}[language=bash]
gmtb_scm: libccppphys.so.X.X.X: cannot open shared object file: No such file or directory
\end{lstlisting}
the environment variable \execout{LD\_LIBRARY\_PATH} must be set to
\begin{lstlisting}[language=bash]
export LD_LIBRARY_PATH=$PWD/ccpp/physics:$LD_LIBRARY_PATH
\end{lstlisting}
before running the model.

A netCDF output file is generated in the location specified in the case
configuration file, if the \execout{output\_dir} variable exists in that file. Otherwise an output directory is constructed from the case, suite, and namelist used (if different from the default). All output directories are placed in the \execout{bin} directory. If using a Docker container, all output is copied to the \execout{/home} directory in container space for volume-mounting purposes. Any standard netCDF file viewing or analysis tools may be used to
examine the output file (ncdump, ncview, NCL, etc).

\subsection{Multiple Run Script Usage}\label{subsection: multirunscript}

A second Python script is provided for automating the execution of multiple integrations through repeated calling of the single run script. From the run directory, one may use this script through the following interface.

\begin{lstlisting}[language=bash]
./multi_run_gmtb_scm.py {[-c CASE_NAME] [-s SUITE_NAME] [-f PATH_TO_FILE]} [-v{v}] [-t] [-d]
\end{lstlisting}

No arguments are required for this script. The \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options form a mutually-exclusive group, so exactly one of these is allowed at one time. If \execout{--c} is specified with a case name, the script will run a set of integrations for all supported suites (defined in \execout{../src/supported\_suites.py}) for that case. If \execout{-s} is specified with a suite name, the script will run a set of integrations for all supported cases (defined in \execout{../src/supported\_cases.py}) for that that suite. If \execout{-f} is specified with the path to a filename, it will read in lists of cases, suites, and namelists to use from that file. An example for this file's syntax can be found in \execout{../src/example\_multi\_run.py}. If multiple namelists are specified in the file, there either must be one suite specified \emph{or} the number of suites must match the number of namelists. If none of the \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options group is specified, the script will run through all permutations of supported cases and suites (as defined in the files previously mentioned).

In addition to the main options, some helper options can also be used with any of those above. The \execout{-v{v} or --verbose} option can be used to output more information from the script to the console and to a log file. If this option is not used, only completion progress messages are written out. If one \execout{-v} is used, the script will write out completion progress messages and all messages and output from the single run script. If two \execout{-vv} are used, the script will also write out all messages and single run script output to a log file (\execout{multi\_run\_gmtb\_scm.log}) in the \execout{bin} directory. The option, \execout{-t or --timer}, can be used to output the elapsed time for each integration executed by the script. Note that the execution time includes file operations performed by the single run script in addition to the execution of the underlying (Fortran) SCM executable. By default, this option will execute one integration of each subprocess. Since some variability is expected for each model run, if greater precision is required, the number of integrations for timing averaging can be set through an internal script variable. This option can be useful, for example, for getting a rough idea of relative computational expense of different physics suites. Finally, the \execout{-d} flag is required when running this command in a Docker container in order to successfully mount a volume between the host machine and the Docker container instance and to share the output and plots with the host machine.

\subsection{Batch Run Script}

If using the model on HPC resources and significant amounts of processor time is anticipated for the experiments, it will likely be necessary to submit a job through the HPC's batch system. An example script has been included in the repository for running the model on Hera's batch system (SLURM). It is located in \execout{gmtb-scm/scm/etc/gmtb\_scm\_slurm\_example.py}. Edit the \execout{job\_name}, \execout{account}, etc. to suit your needs and copy to the \execout{bin} directory. The case name to be run is included in the \execout{command} variable. To use, invoke
\begin{lstlisting}[language=bash]
./gmtb_scm_slurm_example.py
\end{lstlisting}
from the \execout{bin} directory.

Additional details regarding the SCM may be found in the remainder of this guide. More information on the CCPP can be found in the CCPP Technical Documentation available at \url{https://dtcenter.org/GMTB/v4.0/ccpp\_tech\_guide/}.

\section{Creating and Using a Docker Container with SCM and CCPP}
\label{docker}

This guide is not intended as a source for installing the Docker software on a given machine and assumes a working instance. However, the following tips were acquired during a recent installation of Docker on a machine with Windows 10 Home Edition. Further help should be obtained from your system administrator or, lacking other resources, an internet search.

\begin{itemize}
\item Windows 10 Home Edition does not support Docker Desktop due to lack of ``Hyper-V'' support, but does work with Docker Toolbox. See the installation guide (\url{https://docs.docker.com/toolbox/toolbox_install_windows/}).
\item You may need to turn on your CPU's hardware virtualization capability through your system's BIOS.
\item After a successful installation of Docker Toolbox, starting with Docker Quickstart may result in the following error even with virtualization correctly enabled: \execout{This computer doesn’t have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory}. We were able to bypass this error by opening a bash terminal installed with Docker Toolbox, navigating to the directory where it was installed, and executing the following command:
\begin{lstlisting}[language=bash]
docker-machine create default --virtualbox-no-vtx-check
\end{lstlisting}
\end{itemize}

\subsection{Building the Docker image}

A Docker image containing the SCM, CCPP, and its software prerequisites can be generated from the code in the software repository obtained by following section \ref{obtaining_code} by executing the following steps:

NOTE: Windows users can execute these steps in the terminal application that was installed as part of Docker Toolbox.

\begin{enumerate}
\item Navigate to the \execout{gmtb-scm/docker} directory.
\item Run the \execout{docker build} command to generate the Docker image, using the supplied Dockerfile.
\begin{lstlisting}[language=bash]
docker build -t ccpp-scm .
\end{lstlisting}
Inspect the Dockerfile if you would like to see details for how the image is built. The image will contain SCM prerequisite software from DTC, the SCM and CCPP code, and a pre-compiled executable for the SCM with the 6 supported suites for the SCM. A successful build will show two images: dtcenter/common-community-container, and ccpp-scm. To list images, type:
\begin{lstlisting}[language=bash]
docker images
\end{lstlisting}
\end{enumerate}

\subsection{Running the Docker image}

NOTE: Windows users can execute these steps through the Docker Quickstart application installed with Docker Toolbox.

\begin{enumerate}
\item Set up a directory that will be shared between the host machine and the Docker container. When set up correctly, it will contain output generated by the SCM within the container for manipulation by the host machine. For Mac/Linux,
\begin{lstlisting}[language=bash]
mkdir -p /path/to/output
\end{lstlisting}
For Windows, you can try to create a directory of your choice to mount to the container, but it may not work, depending on your particular Docker installation. In our case, we needed to use the home directory of the current user in order to successfully mount the directory (\execout{//C/Users/my username}), so there might be no need to create a new directory.
\item Set an environment variable to use for your SCM output directory. For \textit{t/csh} shells,
\begin{lstlisting}[language=bash]
setenv OUT_DIR /path/to/output
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
export OUT_DIR=/path/to/output
\end{lstlisting}
\item To run the SCM, you can run the Docker container that was just created and give it the same run commands as discussed in sections \ref{subsection: singlerunscript} and \ref{subsection: multirunscript}. \textbf{Be sure to remember to include the \execout{-d} option for all run commands}. For example,
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./run_gmtb_scm.py -c twpice -d
\end{lstlisting}
will run through the TWPICE case using the default suite and namelist and put the output in the shared directory. NOTE: Windows users may need to omit the curly braces around environment variables: use \execout{\$OUT\_DIR} instead of \execout{\$\{OUT\_DIR\}}. For running through all supported cases and suites, use
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./multi_run_gmtb_scm.py -d
\end{lstlisting}
The options included in the above \execout{run} commands are the following:
\begin{itemize}
\item \execout{$--$rm} removes the container when it exits
\item \execout{-it} interactive mode with terminal access
\item \execout{-v} specifies the volume mount from host directory (outside container) to inside the container. Using volumes allows you to share data between the host machine and container. For running the SCM, the output is being mounted from \execout{/home} inside the container to the \execout{OUT\_DIR} on the host machine. Upon exiting the container, data mounted to the host machine will still be accessible.
\item \execout{$--$name} names the container. If no name is provided, the daemon will autogenerate a random string name.
\end{itemize}
\item To use the SCM interactively, run non-default configurations, create plots, or even develop code, issue the following command:
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm /bin/bash
\end{lstlisting}
You will be placed within the container space and within the \execout{bin} directory of the SCM with a pre-compiled executable. At this point, one could use the run scripts as described in previous sections (remembering to include the \execout{-d} option on run scripts if output is to be shared with the host machine). To create plots, from within the \execout{bin} directory of the SCM in container space, issue the following command, with an appropriately configured \execout{*.ini} file, .i.e.
\begin{lstlisting}[language=bash]
./gmtb_scm_analysis.py twpice_all_suites.ini -d
\end{lstlisting}
NOTE: If developing, since the container is ephemeral, one should push their changes to a remote git repository to save them (i.e. a fork on GitHub.com).
\end{enumerate}



